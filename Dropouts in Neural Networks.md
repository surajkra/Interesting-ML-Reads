
## Key Ideas from the paper
### Paper Link:- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting [1]](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)
-> Key idea is to randomly drop units (along with their connections) from the neural network during training which prevents units from co-adapting too much.
![Dropouts](https://github.com/surajkra/Interesting-ML-Reads/blob/master/Images/Dropouts_Architecture.JPG)
-> It prevents overfitting & provides a way of approximately combining exponentially many different Neural Net models



## Important Aspects 

## Interesting Links on Dropouts -
1) [Learning Less to Learn Better [2]](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)

## References

